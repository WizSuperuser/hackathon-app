{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "MODEL_ID=\"gemini-1.5-flash-001\"\n",
    "PROJECT_ID=\"genai-exchange-hackathon\"\n",
    "REGION=\"asia-south1\"\n",
    "\n",
    "llm2 = ChatVertexAI(model_name=MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import vertexai\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "vertexai.init(\n",
    "        project=os.environ.get(\"VERTEX_PROJECT_ID\"),\n",
    "        location=os.environ.get(\"VERTEX_PROJECT_LOCATION\")\n",
    "        )\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-004\", location=REGION, project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "def get_vector_retriever():\n",
    "    QDRANT_URL=os.environ.get(\"QDRANT_URL\")\n",
    "    QDRANT_API_KEY=os.environ.get(\"QDRANT_API_KEY\")\n",
    "    vectorstore = QdrantVectorStore.from_existing_collection(\n",
    "        collection_name=\"dsa_notes\",\n",
    "        embedding=embeddings,\n",
    "        url=QDRANT_URL,\n",
    "        api_key=QDRANT_API_KEY,\n",
    "    )\n",
    "    return vectorstore.as_retriever(k=2)\n",
    "\n",
    "retriever = get_vector_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_search = TavilySearchResults(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from operator import add\n",
    "# from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "    context: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "simple_solver_prompt = \"\"\"You are helping with solving a student's questions about data structures and algorithms.\n",
    "\n",
    "These can be questions about definitions, help with debugging code or hard leetcode style problems to solve.\n",
    "\n",
    "Think carefully before answering any question. Explain your reasoning.\n",
    "\n",
    "Do not hallucinate. Do not make up facts. If you don't know how to answer a problem, just say so.\n",
    "\n",
    "Be concise.\"\"\"\n",
    "\n",
    "def simple_solver(state: State):\n",
    "    \n",
    "    messages = [SystemMessage(content=simple_solver_prompt)]\n",
    "    \n",
    "    # get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    \n",
    "    # if there is summary, we add it to prompt\n",
    "    if summary:\n",
    "        \n",
    "        # add summary to system message\n",
    "        summary_message = f\"Summary of conversation earlier: {summary}\"\n",
    "        \n",
    "        # append summary to any newer message\n",
    "        messages += [HumanMessage(content=summary_message)]\n",
    "    \n",
    "    messages += state[\"messages\"]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    # NEED TO PREVENT CONTEXT FROM BALLOONING if I change it to list and want to persist that\n",
    "    return {\"context\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "socratic_prompt = \"\"\"You are a tutor trying to help a student gain a very strong understanding of a concept/problem. \n",
    "\n",
    "You are helping them with a problem and want to help them understand the concepts by figuring out the solution themselves with only nudges in the right direction.\n",
    "\n",
    "You have the solution above but the student has never seen it.\n",
    "\n",
    "If the student wants to learn about a new concept: use the solution to provide the necessary context. Then, based on that ask the student a question that requires them to apply the concept in code to help enhance their understanding.\n",
    "\n",
    "If the question is a problem to solve: based on the solution to the question, use the socratic method to guide the student towards the answer.\n",
    "\n",
    "Provide hints or prompt the student to think of the next step. If the student seems to be really stuggling with a concept, provide a larger hint. Always take a code-first approach when explaining, giving examples, or solving a problem.\"\"\"\n",
    "\n",
    "def socratic(state: State):\n",
    "    messages = [SystemMessage(content=state[\"context\"] + socratic_prompt)]\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        summary_message = f\"Summary of your conversation with the student: {summary}\"\n",
    "        messages += [HumanMessage(content=summary_message)]\n",
    "    messages += state[\"messages\"]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    \n",
    "    if summary:\n",
    "        summary_message = (\n",
    "            f\"This is the summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Be concise and extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "    \n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "def should_summarize(state: State):\n",
    "    \"\"\"Return whether to summarize depending on length of messages\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    if len(messages) > 6:\n",
    "        return \"summarize\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.errors import NodeInterrupt\n",
    "from langchain_core.messages import AIMessage\n",
    "# Human in the loop\n",
    "def give_answer(state: State):\n",
    "    \"\"\"Ask the student if they want a complete solution\"\"\"\n",
    "    messages = state['messages']\n",
    "    if len(messages) > 2:\n",
    "        state[\"messages\"] += [AIMessage(\"Would you like me to provide you the complete answer? Please reply as yes or no.\")]\n",
    "        raise NodeInterrupt(f\"Recent chat longer than 6 messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_answer(state: State):\n",
    "    if len(state['messages']) > 2:\n",
    "        message = state['messages'][-1]\n",
    "        if 'yes' in str(message).lower():\n",
    "            state[\"messages\"] += [HumanMessage(\"yes\")] + [HumanMessage(state['context'])]\n",
    "            return 'summarize'\n",
    "    else:\n",
    "        return 'socratic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"solver\", simple_solver)\n",
    "workflow.add_node(\"socratic\", socratic)\n",
    "workflow.add_node(\"summarize\", summarize_conversation)\n",
    "# workflow.add_node(\"interrupt\", give_answer)\n",
    "\n",
    "workflow.add_edge(START, \"solver\")\n",
    "workflow.add_edge(\"solver\", \"socratic\")\n",
    "# workflow.add_edge(\"solver\", \"interrupt\")\n",
    "# workflow.add_conditional_edges(\"interrupt\", route_to_answer, {\"summarize\": \"summarize\", \"socratic\": \"socratic\"})\n",
    "workflow.add_conditional_edges(\"socratic\", should_summarize, {\"summarize\": \"summarize\", END: END})\n",
    "workflow.add_edge(\"summarize\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hashing is a fundamental concept in computer science, but don't worry if you're not familiar with it yet. Let's break it down step by step.\n",
      "\n",
      "Imagine you have a large library with millions of books, and you want to find a specific book quickly. One way to do this is by using a catalog system where each book is assigned a unique identifier or \"hash\" based on its title, author, or other characteristics.\n",
      "\n",
      "In computer science, a hash is a fixed-size value that represents a larger piece of data, like a string or an integer. This hash value is generated using a hash function, which takes the input data and maps it to a unique hash value.\n",
      "\n",
      "Here's an example to illustrate this concept. Let's say we have a simple hash function that takes a string as input and returns a hash value based on the length of the string.\n",
      "\n",
      "```python\n",
      "def simple_hash(input_string):\n",
      "    return len(input_string)\n",
      "\n",
      "print(simple_hash(\"hello\"))  # Output: 5\n",
      "print(simple_hash(\"world\"))  # Output: 5\n",
      "```\n",
      "\n",
      "In this example, the hash function `simple_hash` takes a string as input and returns its length as the hash value.\n",
      "\n",
      "Now, let's think about how we can use this concept to solve a problem. Suppose we want to create a program that stores a collection of unique strings, and we want to ensure that each string is unique.\n",
      "\n",
      "How would you design a hash function to store these strings in a way that allows us to quickly check if a string already exists in the collection?\n",
      "\n",
      "What would be the key characteristics of this hash function, and how would you implement it in code?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Don't worry, it's a complex concept. Let's break it down further.\n",
      "\n",
      "To design a hash function that stores unique strings, we need to consider a few key characteristics:\n",
      "\n",
      "1. **Deterministic**: Given a specific input, the hash function should always return the same output.\n",
      "2. **Non-injective**: Different inputs can have the same output (this is known as a collision).\n",
      "3. **Fixed output size**: The output of the hash function should always be the same size, regardless of the input size.\n",
      "\n",
      "Let's focus on the first characteristic: deterministic. What does it mean for a function to be deterministic?\n",
      "\n",
      "In the context of a hash function, being deterministic means that if we input the same string, we should always get the same output. This is important because it ensures that we can consistently map a string to a specific hash value.\n",
      "\n",
      "Here's an example to illustrate this concept:\n",
      "\n",
      "```python\n",
      "def hash_function(input_string):\n",
      "    # What code would we need to write here to make this function deterministic?\n",
      "    pass\n",
      "```\n",
      "\n",
      "To make this function deterministic, we need to ensure that the output depends only on the input string, and not on any other factors. Can you think of a way to do this?\n",
      "\n",
      "For example, how about using the ASCII values of the characters in the input string? We could sum up the ASCII values, or multiply them together, or...?\n",
      "\n",
      "What do you think?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Let's take a closer look at the idea of using ASCII values to make the hash function deterministic.\n",
      "\n",
      "In ASCII, each character has a unique numerical value. For example, the character 'a' has an ASCII value of 97, and the character 'b' has an ASCII value of 98.\n",
      "\n",
      "We can use these ASCII values to create a deterministic hash function by summing up the values of all the characters in the input string.\n",
      "\n",
      "Here's an example:\n",
      "\n",
      "```python\n",
      "def hash_function(input_string):\n",
      "    hash_value = 0\n",
      "    for char in input_string:\n",
      "        hash_value += ord(char)\n",
      "    return hash_value\n",
      "```\n",
      "\n",
      "In this code, we use the `ord` function to get the ASCII value of each character, and then add it to the `hash_value`.\n",
      "\n",
      "Let's test this function with a few examples:\n",
      "\n",
      "```python\n",
      "print(hash_function(\"hello\"))  # Output: 532\n",
      "print(hash_function(\"world\"))  # Output: 552\n",
      "print(hash_function(\"hello\"))  # Output: 532 (same as before)\n",
      "```\n",
      "\n",
      "As you can see, the function is deterministic because it always returns the same output for the same input.\n",
      "\n",
      "However, there's still a problem with this function. Can you think of what it is?\n",
      "\n",
      "Hint: Think about what happens when we input two different strings that have the same characters, but in a different order. For example, what would happen if we input \"hello\" and \"olleh\"?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"6\"}}\n",
    "\n",
    "input_message = HumanMessage(content=\"what is hashing?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "    \n",
    "input_message = HumanMessage(content=\"I'm not sure\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "    \n",
    "input_message = HumanMessage(content=\"I think I'm starting to get it but I need more help\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values.get(\"summary\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Would you like me to provide you the complete answer? Please reply as yes or no.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"6\"}}\n",
    "input_message = HumanMessage(content=\"I think the databases have a key associated with each item that can be indexed very quickly.\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('interrupt',)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PregelTask(id='04f3cc1c-d4a0-c3c5-c41f-4fdfeac8f9c3', name='interrupt', error=None, interrupts=(Interrupt(value='Recent chat longer than 6 messages', when='during'),), state=None),)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '6',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1ef7b188-064c-6666-8007-8aea05227951'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.update_state(\n",
    "    config,\n",
    "    {\"messages\": \"yes\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is hashing?', id='8c23a2f7-86bc-4a1c-a8a4-e307648abdd2'), AIMessage(content=\"Hashing is a fundamental concept in computer science, but let's break it down step by step.\\n\\nTo start, can you think of a situation where you have a large amount of data, and you want to quickly identify or locate a specific piece of information within that data? Perhaps you've used a library or a database before. How do you think the library or database manages to find the specific book or piece of information you're looking for so quickly?\\n\\nWhat if I told you that there's a way to map a piece of information, like a string or an integer, to a unique identifier that can help us locate it more efficiently? What would you call this process, and how do you think it might work?\", response_metadata={'token_usage': {'completion_tokens': 145, 'prompt_tokens': 515, 'total_tokens': 660, 'completion_time': 0.581719616, 'prompt_time': 0.139046958, 'queue_time': 0.55903782, 'total_time': 0.720766574}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b6828be2c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-eff18597-4012-46eb-97ec-0dcf4d8a1344-0', usage_metadata={'input_tokens': 515, 'output_tokens': 145, 'total_tokens': 660}), HumanMessage(content='I think the databases have a key associated with each item that can be indexed very quickly.', id='429f9880-275e-422e-bca0-140e684d3fb5'), HumanMessage(content='yes', id='efcb259d-2a73-49d6-bf0a-c5885f41328d'), HumanMessage(content='no', id='e24a0ab9-6eee-4ea6-81b6-aaffade6243f')], 'context': \"That's a great intuition. In a database, a key is often used to uniquely identify a piece of information. And you're correct that databases use indexes to quickly locate data.\\n\\nHashing is a process that maps a piece of information, like a string or an integer, to a unique identifier, often called a hash code or digest. This hash code serves as an index or a key that can be used to quickly locate the associated data.\\n\\nThink of it like a phonebook. In a phonebook, names are mapped to phone numbers. When you look up a name, you can quickly find the corresponding phone number. Similarly, in hashing, a piece of data (like a string) is mapped to a unique hash code, which can be used to quickly locate the associated data.\\n\\nA good hashing function should have a few key properties:\\n\\n1. **Deterministic**: Given the same input, the hashing function should always produce the same output.\\n2. **Non-injective**: Different inputs should ideally produce different outputs (this is where collisions can occur).\\n3. **Fixed output size**: The output of the hashing function should always be of a fixed size, regardless of the input size.\\n\\nHashing has many applications, including data storage, data retrieval, and even cryptography. Some common data structures that rely on hashing include hash tables, sets, and maps.\\n\\nDo you have any questions about how hashing functions work or any specific applications you'd like to know more about?\"}\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream(None, config, stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('interrupt',)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '6',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1ef7b484-199a-6b88-8008-d6e7871d3b09'}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.update_state(config, {\"messages\": \"no\"},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'socratic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is pretty_print in python?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]}, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mevent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msocratic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpretty_print()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'socratic'"
     ]
    }
   ],
   "source": [
    "for event in graph.stream({\"messages\": [HumanMessage(content=\"what is pretty_print in python?\")]}, config, stream_mode=\"updates\"):\n",
    "    event['socratic']['messages'].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages\n",
      "context\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream(None, config, stream_mode=\"values\"):\n",
    "    for value in event:\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream({\"messages\": input_message}, config, stream_mode=\"updates\"):\n",
    "    chunk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
